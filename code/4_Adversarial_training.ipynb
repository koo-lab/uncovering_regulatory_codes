{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from deepomics import neuralnetwork as nn\n",
    "from deepomics import utils, fit, visualize, saliency, metrics\n",
    "\n",
    "import copy\n",
    "import helper\n",
    "import time\n",
    "\n",
    "np.random.seed(247)\n",
    "tf.set_random_seed(247)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/Synthetic_dataset.h5'\n",
    "results_path = '../results'\n",
    "\n",
    "# load dataset\n",
    "train, valid, test = helper.load_synthetic_dataset(data_path)\n",
    "\n",
    "test_model = helper.load_synthetic_models(data_path, dataset='test')\n",
    "    \n",
    "# get data shapes\n",
    "input_shape = list(train['inputs'].shape)\n",
    "input_shape[0] = None\n",
    "output_shape = [None, train['targets'].shape[1]]\n",
    "\n",
    "true_index = np.where(test['targets'][:,0] == 1)[0]\n",
    "X = test['inputs'][true_index]\n",
    "X_model = test_model[true_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def perturb(x_nat, y, sess, nnmodel, feed_dict, grad_tensor, k=20):\n",
    "    \"\"\"Given a set of examples (x_nat, y), returns a set of adversarial\n",
    "       examples within epsilon of x_nat in l_infinity norm.\"\"\"\n",
    "    epsilon = 0.2\n",
    "    x = np.copy(x_nat)\n",
    "    feed_dict[xx] = x\n",
    "    feed_dict[yy] = y\n",
    "    feed_dict[is_train] = False\n",
    "    \n",
    "    for i in range(k):\n",
    "        feed_dict[xx] = x\n",
    "        grad = sess.run(grad_tensor, feed_dict=feed_dict)\n",
    "\n",
    "        x += 0.1 /(i+10) * np.sign(grad)\n",
    "\n",
    "        x = np.clip(x, x_nat - epsilon, x_nat + epsilon) \n",
    "        x = np.clip(x, 0, 1) # ensure valid pixel range\n",
    "\n",
    "    feed_dict[is_train] = True\n",
    "    return x\n",
    "\n",
    "def initialize_feed_dict(placeholders, feed_dict):\n",
    "\n",
    "    train_feed = {}\n",
    "    for key in feed_dict.keys():\n",
    "        train_feed[placeholders[key]] = feed_dict[key]\n",
    "    return train_feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_models = ['LocalNet', 'DistNet'] #\n",
    "dropout_status = [True, False]#,  True,   False,  False,  True,  False]\n",
    "l2_status =      [True, False]#,  False,  True,   False,  False, True]\n",
    "bn_status =      [True, False]#, False,  False,  True,   True,  True]\n",
    "\n",
    "params_path = utils.make_directory(results_path, 'model_params')\n",
    "\n",
    "batch_size = 50\n",
    "verbose = 1 \n",
    "num_epochs = 50\n",
    "num_clean_epochs = 20\n",
    "print_adv_test = True\n",
    "\n",
    "adv_test = copy.deepcopy(test)\n",
    "\n",
    "# get data shapes\n",
    "input_shape = list(train['inputs'].shape)\n",
    "input_shape[0] = None\n",
    "output_shape = [None, train['targets'].shape[1]]\n",
    "\n",
    "\n",
    "# loop through models\n",
    "for model_name in all_models:\n",
    "    print('model: ' + model_name)\n",
    "\n",
    "    for i in range(len(dropout_status)):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # compile neural trainer\n",
    "        name = model_name\n",
    "        if dropout_status[i]:\n",
    "            name += '_do'\n",
    "        if l2_status[i]:\n",
    "            name += '_l2'\n",
    "        if bn_status[i]:\n",
    "            name += '_bn'\n",
    "        name += '_adv'\n",
    "        file_path = os.path.join(params_path, name)\n",
    "\n",
    "        # load model parameters\n",
    "        model_layers, optimization, _ = helper.load_model(model_name, \n",
    "                                                          input_shape,\n",
    "                                                          dropout_status[i], \n",
    "                                                          l2_status[i], \n",
    "                                                          bn_status[i])\n",
    "\n",
    "\n",
    "        # build neural network class\n",
    "        nnmodel = nn.NeuralNet()\n",
    "        nnmodel.build_layers(model_layers, optimization, supervised=True)\n",
    "\n",
    "        grad_tensor = tf.gradients(nnmodel.mean_loss, nnmodel.placeholders['inputs'])[0]\n",
    "\n",
    "        xx = nnmodel.placeholders['inputs']\n",
    "        yy = nnmodel.placeholders['targets']\n",
    "        is_train = nnmodel.placeholders['is_training']\n",
    "        loss = nnmodel.mean_loss\n",
    "        #   nnmodel.inspect_layers()\n",
    "        performance = nn.MonitorPerformance('train', optimization['objective'], verbose)\n",
    "        performance.set_start_time(start_time = time.time())\n",
    "\n",
    "        train_calc = [nnmodel.train_step, loss, nnmodel.metric]\n",
    "        train_feed = initialize_feed_dict(nnmodel.placeholders, nnmodel.feed_dict)\n",
    "\n",
    "        # create neural trainer\n",
    "        file_path = os.path.join(params_path, model_name)\n",
    "        nntrainer = nn.NeuralTrainer(nnmodel, save='best', file_path=file_path)\n",
    "\n",
    "        # initialize session\n",
    "        sess = utils.initialize_session()\n",
    "\n",
    "        # set data in dictionary\n",
    "        #   data = {'train': train, 'valid': valid, 'test': test}\n",
    "        x_train = train['inputs']\n",
    "        y_train = train['targets']\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch: %d\"%(epoch))\n",
    "            index = np.random.permutation(len(x_train))\n",
    "\n",
    "            for i in range((len(x_train) // batch_size)):\n",
    "                # batch\n",
    "                clean_batch_x = x_train[index[i*batch_size:(i+1)*batch_size]]\n",
    "                clean_batch_y = y_train[index[i*batch_size:(i+1)*batch_size]]\n",
    "\n",
    "\n",
    "                if epoch >= num_clean_epochs:\n",
    "                    adv_batch = perturb(clean_batch_x, clean_batch_y,\n",
    "                                sess, nnmodel, train_feed, grad_tensor)\n",
    "\n",
    "\n",
    "                    train_feed[xx] = np.concatenate([clean_batch_x, adv_batch])\n",
    "                    train_feed[yy] = np.concatenate([clean_batch_y, clean_batch_y])\n",
    "                else:\n",
    "                    train_feed[xx] = clean_batch_x\n",
    "                    train_feed[yy] = clean_batch_y\n",
    "\n",
    "                results = sess.run(train_calc, feed_dict=train_feed)\n",
    "                performance.add_loss(results[1])\n",
    "                #performance.progress_bar(i+1., (len(x_train) // batch_size), metric/(i+1))\n",
    "\n",
    "            # print validation accuracy\n",
    "            predictions = nntrainer.get_activations(sess, valid, 'output')\n",
    "            print(metrics.accuracy(valid['targets'], predictions))\n",
    "\n",
    "            if print_adv_test and epoch >= num_clean_epochs:\n",
    "                adv_test['inputs'] = perturb(test['inputs'], test['targets'], sess, nnmodel, train_feed, grad_tensor)\n",
    "        #       adv_test['inputs'] = test['inputs']\n",
    "                predictions = nntrainer.get_activations(sess, adv_test, 'output')\n",
    "                roc, roc_curves = metrics.roc(test['targets'], predictions)\n",
    "                #print('Adversarial AUC')\n",
    "                #print np.mean(roc)\n",
    "                print('Adversarial Accuracy')\n",
    "                print(metrics.accuracy(test['targets'], predictions))\n",
    "\n",
    "            # save cross-validcation metrics\n",
    "            loss, mean_vals, error_vals = nntrainer.test_model(sess, valid,\n",
    "                                                                    name=\"valid\",\n",
    "                                                                    batch_size=batch_size,\n",
    "                                                                    verbose=verbose)\n",
    "\n",
    "        nnmodel.save_model_parameters(sess, os.path.join(params_path, model_name, name+'.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on held-out test set\n",
    "loss, mean_vals, error_vals = nntrainer.test_model(sess, test,\n",
    "                                                        name=\"test\",\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['backprop', 'smoothgrad']\n",
    "for method in methods:\n",
    "\n",
    "    backprop_results ={}\n",
    "    for model_name in all_models:\n",
    "\n",
    "        for i in range(len(dropout_status)):\n",
    "\n",
    "            # compile neural trainer\n",
    "            name = model_name\n",
    "            if dropout_status[i]:\n",
    "                name += '_do'\n",
    "            if l2_status[i]:\n",
    "                name += '_l2'\n",
    "            if bn_status[i]:\n",
    "                name += '_bn'\n",
    "            name += '_adv'\n",
    "            file_path = os.path.join(params_path, name)\n",
    "\n",
    "            # attribution parameters\n",
    "            params = {'model_name': model_name, \n",
    "                      'input_shape': input_shape, \n",
    "                      'dropout_status': dropout_status[i],\n",
    "                      'l2_status': l2_status[i],\n",
    "                      'bn_status': bn_status[i],\n",
    "                      'model_path': file_path+'.ckpt',\n",
    "                     }\n",
    "\n",
    "            # calculate attribution scores\n",
    "            if method == 'smoothgrad':\n",
    "                attribution_score = helper.smooth_backprop(X, params, layer='output', class_index=None, num_average=50)\n",
    "            else:\n",
    "                attribution_score = helper.backprop(X, params, layer='output', class_index=None, method=method)\n",
    "\n",
    "            # calculate TPS and FPS \n",
    "            info = []\n",
    "            for j, gs in enumerate(attribution_score):\n",
    "                X_saliency = np.squeeze(gs).T\n",
    "                tps, fps = helper.entropy_weighted_distance(X_saliency, X_model[j])\n",
    "                info.append([tps, fps])\n",
    "            info = np.array(info)\n",
    "\n",
    "            backprop_results[name] = info\n",
    "\n",
    "    # save results\n",
    "    with open(os.path.join(results_path, method+'_adv.pickle'), 'wb') as f:\n",
    "        cPickle.dump(backprop_results, f, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
